# -*- coding: utf-8 -*-
"""Netflix Rec Sys.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fsAfzxvYLrVy5G6te2Kr1jlKS282S8--

# **Netflix Recommendation System**
"""

from google.colab import drive
drive.mount('/content/drive')

"""# **Importing Libraries**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""# **Loading the data set**"""

import zipfile
import pandas as pd

# Extract the file from the zip archive
with zipfile.ZipFile('/content/drive/MyDrive/Copy of combined_data_1.txt.zip', 'r') as zip_ref:
    zip_ref.extractall('/content/drive/MyDrive/extracted_data') # extracts to a folder named extracted_data

# Now read the extracted file
# Assuming the extracted file is named 'combined_data_1.txt'
df = pd.read_csv('/content/drive/MyDrive/extracted_data/combined_data_1.txt')

#OR

#Directly reading zipped file.
df = pd.read_csv('/content/drive/MyDrive/Copy of combined_data_1.txt.zip', compression='zip') #reading the zipped file directly.

df = pd.read_csv('/content/drive/MyDrive/Copy of combined_data_1.txt.zip', compression='zip', header=None, names=['Cust_Id', 'Rating'], usecols=[0,1])
df

df.dtypes

df['Rating'] = df['Rating'].astype(float)

df.dtypes

df.shape

df.info()

"""#  **Finding the distribution of ratings**


"""

distRating = df.groupby('Rating')['Rating'].count()
distRating

"""# **Finding the number of movies**"""

moviecount = df.isnull().sum()[1]
moviecount

"""# **Finding the number of ratings**"""

ratingcount = df['Cust_Id'].count() - moviecount
print('Total number of ratings irrespective of duplicate ratings is', ratingcount)

"""# **Finding the number of unique users**"""

userscount = df['Cust_Id'].nunique()-moviecount
print('Total number of unique users is', userscount)

"""# **Ploting the distribution of rating.**"""

fig = distRating.plot(kind='barh',legend=False, figsize=(15,10))
plt.title(f'Total pool :{moviecount} movies, {userscount} users, {ratingcount} ratings')

"""# **Creating an array containing the movie IDs with respect to the rows of ratings**"""

df_NAN = pd.DataFrame(pd.isnull(df.Rating))
df_NAN.value_counts()

df_NAN = df_NAN[df_NAN['Rating'] == True]
df_NAN.shape

"""# **Resetting the index**"""

df_NAN= df_NAN.reset_index()
df_NAN

"""# **Creating a numpy array for movies**"""

movie_np = []
movie_id = 1

for i, j in zip(df_NAN['index'][1:], df_NAN['index'][:-1]):
    # numpy approach
    temp = np.full((1,i-j-1), movie_id)
    movie_np = np.append(movie_np, temp)
    movie_id += 1

#adding the lastly left records
last_record = np.full((1,len(df) - df_NAN.iloc[-1, 0] - 1),movie_id)
movie_np = np.append(movie_np, last_record)

print(f'Movie numpy: {movie_np}')
print(f'Length: {len(movie_np)}')

"""# **Combining the data together**"""

import warnings
warnings.filterwarnings('ignore')
df = df[pd.notnull(df['Rating'])]
df['Movie_Id'] = movie_np.astype('int')
df['Cust_Id'] = df['Cust_Id'].astype('int')
print('The merged data set')
df.head()

df.tail()



"""# **Data cleaning**"""

# Droping the movies with minimum review
f = ['count','mean']
df_movie_summary = df.groupby('Movie_Id')['Rating'].agg(f)
df_movie_summary.index = df_movie_summary.index.map(int)
movie_benchmark = round(df_movie_summary['count'].quantile(0.7),0)
drop_movie_list = df_movie_summary[df_movie_summary['count'] < movie_benchmark].index

print(f'Movies with minimun reviews are {movie_benchmark}')

# Droping the customer with minimum review
df_cust_summary = df.groupby('Cust_Id')['Rating'].agg(f)
df_cust_summary.index = df_cust_summary.index.map(int)
cust_benchmark = round(df_cust_summary['count'].quantile(0.7),0)
drop_cust_list = df_cust_summary[df_cust_summary['count'] < cust_benchmark].index
print(f'Customer with minimun reviews are {cust_benchmark}')

df.shape

"""# **Dropping inactive customers and least rated movies**"""

df = df[~df['Movie_Id'].isin(drop_movie_list)]
df = df[~df['Cust_Id'].isin(drop_cust_list)]
print('The number of rows of the new dataframe is', df.shape)

df

"""# **Creating rating matrix using pivot table**"""

df.pivot = pd.pivot_table(df,values='Rating',index='Cust_Id',columns='Movie_Id')
df.pivot.head()

df.pivot.shape

"""# **Loading title data set**"""

df_title = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Copy of movie_titles.csv', encoding='latin-1', on_bad_lines='skip', sep=',') # Adding  on_bad_lines='skip' to skip problematic lines and setting sep=',' explicitly.

df_title = df_title.iloc[:,:3]
df_title.columns = ['Movie_Id', 'Year', 'Name']
df_title.head()

"""# **Importing sci-kit learn surprise library to implement SVD**"""

!pip install surprise

"""# **Importing libraries**"""

import math
import matplotlib.pyplot as plt
from surprise import Reader, Dataset, SVD
from surprise.model_selection import cross_validate

# Loading thr reader to read the data
reader = Reader()

# consider 100K records for faster compuatation

data = Dataset.load_from_df(df[['Cust_Id', 'Movie_Id', 'Rating']][:100000], reader)

data

# Using SVD algorithm

svd = SVD()

cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)

"""# **Performing the recommendation**"""

df.head()

# Finding the 5 Star rated movies for the user 712664

df_user = df[(df['Cust_Id'] == 712664) & (df['Rating'] == 5)]
df_user = df_user.set_index('Movie_Id')
df_user = df_user.join(df_title)['Name']
df_user.head(10)

"""# **Training the model**"""

user_712664 = df_title.copy()
user_712664 = user_712664.reset_index()
user_712664 = user_712664[~user_712664['Movie_Id'].isin(drop_movie_list)]

user_712664

user_712664.head(10)

